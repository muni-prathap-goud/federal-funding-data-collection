{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Federal Funding Recipients Data Collection System\n",
    "\n",
    "##  Overview\n",
    "**Advanced recipient data collection** from USASpending.gov API using federal account foundation data.\n",
    "\n",
    "This notebook implements a sophisticated **4-stage data pipeline**:\n",
    "1. **Federal Accounts  Recipients**: Expands each federal account into individual recipient records\n",
    "2. **Parallel Processing**: High-performance ThreadPoolExecutor with 50 concurrent workers\n",
    "3. **Idempotent Storage**: Deduplication and incremental append capabilities\n",
    "4. **Intelligent Retry System**: Period-scoped failure recovery with automatic cleanup\n",
    "\n",
    "**Input**: `federal_accounts_*.csv` files (from previous pipeline stages)  \n",
    "**Output**: `recipients_FY{YYYY}_Q{Q}.csv` + `failures_FY{YYYY}_Q{Q}.csv` files\n",
    "\n",
    "**Key Features:**\n",
    "-  **Type-specific API calls**: Uses `type=\"recipient\"` filtering\n",
    "-  **Composite key deduplication**: Prevents duplicate recipient records  \n",
    "-  **Period-scoped organization**: Separate files per fiscal year/quarter\n",
    "-  **Defensive programming**: Robust error handling and file corruption protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_31hx1BXBoSt"
   },
   "outputs": [],
   "source": [
    "#  Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import logging\n",
    "import urllib3\n",
    "\n",
    "#  Suppress urllib3 warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setup & Dependencies\n",
    "\n",
    "**Core libraries** for recipient data collection system:\n",
    "- **`pandas`**: DataFrame operations and CSV processing\n",
    "- **`requests`**: HTTP API calls with session management\n",
    "- **`ThreadPoolExecutor`**: Parallel processing (50 concurrent workers)\n",
    "- **`HTTPAdapter/Retry`**: Custom session configuration\n",
    "- **`logging/urllib3`**: Noise suppression for cleaner output\n",
    "\n",
    "**Key Setup Actions:**\n",
    "-  **Suppresses urllib3 warnings**: Eliminates noisy HTTP connection logs\n",
    "-  **Configures logging levels**: Reduces verbose connection pool messages\n",
    "-  **Imports threading utilities**: Enables high-performance parallel API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1755719945747,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "JsnwBqJOIrnC",
    "outputId": "ff4a192d-ca1c-4ea5-b520-e953c47d98b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Google Drive Integration\n",
    "\n",
    "**Purpose**: Mount Google Drive for Colab environment data persistence\n",
    "\n",
    "**Functionality:**\n",
    "- **Colab Environment**: Mounts Google Drive at `/content/drive/MyDrive/`\n",
    "- **Data Access**: Enables access to federal account CSV files\n",
    "- **Result Storage**: Saves recipient data to Google Drive for persistence\n",
    "- **Cross-Session**: Maintains data across Colab session restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBZDqTVnIQiM"
   },
   "outputs": [],
   "source": [
    "#  Setup session with retry logic\n",
    "def setup_session():\n",
    "    \"\"\"\n",
    "    Creates and configures a session with retry logic for HTTP requests.\n",
    "    Ensures resilience in case of server or network issues.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=0,\n",
    "        backoff_factor=1.0,\n",
    "        status_forcelist=[500, 502, 503, 504],\n",
    "        allowed_methods=[\"POST\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  HTTP Session Configuration\n",
    "\n",
    "**Creates robust HTTP session** with **deliberately disabled retries** (total=0):\n",
    "\n",
    "### Session Strategy\n",
    "**Important**: Despite the docstring mentioning \"retry logic,\" this session is configured with `total=0` retries\n",
    "- **No Automatic Retries**: Relies on application-level retry mechanisms instead\n",
    "- **Manual Control**: Enables precise control over retry behavior at the application layer\n",
    "- **Status Force List**: Prepared to handle 500, 502, 503, 504 errors (but won't retry automatically)\n",
    "\n",
    "### Configuration Details\n",
    "- **HTTPAdapter**: Configured for HTTPS requests\n",
    "- **Backoff Factor**: 1.0 second delays (unused due to total=0)\n",
    "- **Allowed Methods**: POST requests for API calls\n",
    "- **Connection Pooling**: Reuses connections for better performance\n",
    "\n",
    "**Design Choice**: Application handles retries explicitly rather than relying on automatic session-level retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpBuEmjiITQ7"
   },
   "outputs": [],
   "source": [
    "#  Fetch recipient data for a single row using quarters\n",
    "def fetch_recipient(session, row):\n",
    "    \"\"\"\n",
    "    Sends a POST request to the USAspending API to fetch recipient data\n",
    "    for a given federal account record using fiscal year and quarter.\n",
    "    Returns successful records and failure logs.\n",
    "    \"\"\"\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    fy = str(row['fy'])\n",
    "    quarter = str(row['quarter'])\n",
    "    function_code = str(row['budget_function_code']).zfill(3)\n",
    "    subfunction_code = str(row['budget_subfunction_code']).zfill(3)\n",
    "    federal_account_code = str(row['federal_account_code']).zfill(4)\n",
    "\n",
    "    url = \"https://api.usaspending.gov/api/v2/spending/\"\n",
    "    payload = {\n",
    "        \"type\": \"recipient\",\n",
    "        \"filters\": {\n",
    "            \"fy\": fy,\n",
    "            \"quarter\": quarter,\n",
    "            \"budget_function\": function_code,\n",
    "            \"budget_subfunction\": subfunction_code,\n",
    "            \"federal_account\": federal_account_code\n",
    "        }\n",
    "    }\n",
    "\n",
    "    all_records = []\n",
    "    all_failures = []\n",
    "\n",
    "    try:\n",
    "        resp = session.post(url, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        results = data.get(\"results\", [])\n",
    "\n",
    "        for item in results:\n",
    "            all_records.append({\n",
    "                \"fy\": fy,\n",
    "                \"quarter\": quarter,\n",
    "                \"budget_function_code\": function_code,\n",
    "                \"budget_subfunction_code\": subfunction_code,\n",
    "                \"federal_account_code\": federal_account_code,\n",
    "                \"recipient_id\": item.get(\"id\"),\n",
    "                \"recipient_name\": item.get(\"name\"),\n",
    "                \"recipient_code\": item.get(\"code\"),\n",
    "                \"obligated_amount\": item.get(\"amount\"),\n",
    "                \"total_amount\": item.get(\"total\")\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        all_failures.append({\n",
    "            \"fy\": fy,\n",
    "            \"quarter\": quarter,\n",
    "            \"budget_function_code\": function_code,\n",
    "            \"budget_subfunction_code\": subfunction_code,\n",
    "            \"federal_account_code\": federal_account_code,\n",
    "            \"reason\": str(e)\n",
    "        })\n",
    "\n",
    "    return all_records, all_failures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Core Recipient Fetcher - API Collection Engine\n",
    "\n",
    "**Primary data collection function** for recipient data at the **function  subfunction  federal account slice**:\n",
    "\n",
    "### Input Parameters\n",
    "**Expects row with complete hierarchical context:**\n",
    "- `fy`: Fiscal year for data collection\n",
    "- `quarter`: Specific quarter (1-4) for temporal filtering  \n",
    "- `budget_function_code`: 3-digit function code (zero-padded)\n",
    "- `budget_subfunction_code`: 3-digit subfunction code (zero-padded)\n",
    "- `federal_account_code`: 4-digit federal account code (zero-padded)\n",
    "\n",
    "### API Request Structure\n",
    "**Endpoint**: `/api/v2/spending/` with `type=\"recipient\"` filtering\n",
    "**Filters Applied:**\n",
    "- **Temporal**: Fiscal year + quarter combination\n",
    "- **Hierarchical**: Budget function + subfunction + federal account\n",
    "- **Type-Specific**: Recipients only (not awards or other entities)\n",
    "\n",
    "### Response Processing\n",
    "**Builds structured records with composite keys:**\n",
    "- **Primary Key**: `(fy, quarter, budget_function_code, budget_subfunction_code, federal_account_code, recipient_id)`\n",
    "- **Recipient Data**: Name, code, ID from API response\n",
    "- **Financial Data**: Obligated amount + optional total amount\n",
    "- **Error Handling**: Graceful failure capture with detailed reason logging\n",
    "\n",
    "**Rate Limiting**: 0.3 second delay between requests to respect API limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fskmWRZD7IDO"
   },
   "outputs": [],
   "source": [
    "#  Step 1: Read and clean data\n",
    "\n",
    "def read_and_filter_csv(file_path):\n",
    "    \"\"\"\n",
    "    Reads a federal accounts CSV and filters out rows with zero obligated amounts.\n",
    "    Returns a filtered DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    #if \"obligated_amount\" in df.columns:\n",
    "        #df = df[df[\"obligated_amount\"] > 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Input File Processing - Federal Accounts Reader\n",
    "\n",
    "**Defensive CSV reading** for federal account input files:\n",
    "\n",
    "### Input Source\n",
    "**File Pattern**: `federal_accounts_*.csv` files from previous pipeline stages\n",
    "- **Source**: Generated by federal account collection system\n",
    "- **Content**: Federal account records with fiscal year, quarter, and hierarchical codes\n",
    "- **Format**: Structured CSV with consistent column naming\n",
    "\n",
    "### Data Processing Strategy\n",
    "**Current Implementation**: Reads all federal account records without filtering\n",
    "```python\n",
    "# Optional zero-amount filtering (currently commented out):\n",
    "# if \"obligated_amount\" in df.columns:\n",
    "#     df = df[df[\"obligated_amount\"] > 0]\n",
    "```\n",
    "\n",
    "### Design Rationale\n",
    "**Inclusive Approach**: Processes all federal accounts regardless of obligation amounts\n",
    "- **Complete Coverage**: Ensures no recipient data is missed due to zero-obligation federal accounts\n",
    "- **Downstream Filtering**: Allows recipient-level filtering instead of account-level pre-filtering\n",
    "- **Data Integrity**: Maintains complete federal account context for API calls\n",
    "\n",
    "**Output**: Clean DataFrame ready for parallel recipient data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnCkbzg87H_c"
   },
   "outputs": [],
   "source": [
    "#  Step 2: Fetch data from API using ThreadPoolExecutor\n",
    "\n",
    "def fetch_all_recipients(df, max_workers=50):\n",
    "    \"\"\"\n",
    "    Submits all API calls in parallel using a thread pool and returns combined results.\n",
    "    \"\"\"\n",
    "    session = setup_session()\n",
    "    results = []\n",
    "    failures = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(fetch_recipient, session, row) for _, row in df.iterrows()]\n",
    "        for future in as_completed(futures):\n",
    "            res, fail = future.result()\n",
    "            results.extend(res)\n",
    "            failures.extend(fail)\n",
    "\n",
    "    return pd.DataFrame(results), pd.DataFrame(failures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  High-Performance Parallel Processing Engine\n",
    "\n",
    "**Massive parallelization** of recipient data collection with **50 concurrent workers**:\n",
    "\n",
    "### Parallel Processing Strategy\n",
    "**ThreadPoolExecutor Configuration:**\n",
    "- **Max Workers**: 50 concurrent threads (aggressive parallelization)\n",
    "- **Task Distribution**: One API call per federal account row\n",
    "- **Session Sharing**: Single HTTP session across all threads for connection pooling\n",
    "- **Result Aggregation**: Combines all thread results into unified DataFrames\n",
    "\n",
    "### Execution Flow\n",
    "1. **Session Creation**: Single `setup_session()` call for all threads\n",
    "2. **Task Submission**: Each DataFrame row becomes a separate thread task\n",
    "3. **Concurrent Execution**: Up to 50 simultaneous API calls to USASpending.gov\n",
    "4. **Result Collection**: `as_completed()` aggregates results as threads finish\n",
    "5. **Data Separation**: Successful records and failures collected separately\n",
    "\n",
    "### Performance Benefits\n",
    "-  **High Throughput**: 50x speed improvement over sequential processing\n",
    "-  **Connection Reuse**: Session pooling reduces connection overhead\n",
    "-  **Resource Efficiency**: Threads share memory and CPU resources effectively\n",
    "-  **Scalable Design**: Handles large federal account datasets efficiently\n",
    "\n",
    "**Output**: Two DataFrames - successful recipient records and failure logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R6DxUI77H0r"
   },
   "outputs": [],
   "source": [
    "def save_recipient_results(results_df, failures_df, file_path, output_base_folder):\n",
    "    \"\"\"\n",
    "    Saves recipient results and failure logs.\n",
    "\n",
    "    - Appends only unique recipient records (based on key columns).\n",
    "    - Handles empty/corrupted existing result files safely.\n",
    "    - Reports how many new records added vs duplicates skipped.\n",
    "    - Overwrites the failures file each time.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    base_filename = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "    for prefix in [\"federal_accounts_\", \"failures_\"]:\n",
    "        if base_filename.startswith(prefix):\n",
    "            base_filename = base_filename.replace(prefix, \"\")\n",
    "            break\n",
    "\n",
    "    year_quarter = base_filename\n",
    "    os.makedirs(output_base_folder, exist_ok=True)\n",
    "    results_path = os.path.join(output_base_folder, f\"recipients_{year_quarter}.csv\")\n",
    "    failures_path = os.path.join(output_base_folder, f\"failures_{year_quarter}.csv\")\n",
    "\n",
    "    unique_keys = [\n",
    "        \"fy\", \"quarter\", \"budget_function_code\",\n",
    "        \"budget_subfunction_code\", \"federal_account_code\", \"recipient_id\"\n",
    "    ]\n",
    "\n",
    "    # Deduplicate incoming new results\n",
    "    results_df.drop_duplicates(subset=unique_keys, inplace=True)\n",
    "\n",
    "    #  Try to read existing results safely\n",
    "    existing_results = None\n",
    "    if os.path.exists(results_path):\n",
    "        try:\n",
    "            existing_results = pd.read_csv(results_path)\n",
    "            if existing_results.empty or existing_results.columns.size == 0:\n",
    "                print(f\" Ignoring empty/corrupted results file: {results_path}\")\n",
    "                existing_results = None\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\" Ignoring EmptyDataError on: {results_path}\")\n",
    "            existing_results = None\n",
    "        except Exception as e:\n",
    "            print(f\" Skipped reading existing results ({e}): {results_path}\")\n",
    "            existing_results = None\n",
    "\n",
    "    #  Merge results\n",
    "    if existing_results is not None:\n",
    "        before_count = len(existing_results)\n",
    "        combined = pd.concat([existing_results, results_df], ignore_index=True)\n",
    "        combined.drop_duplicates(subset=unique_keys, inplace=True)\n",
    "        added_count = len(combined) - before_count\n",
    "        duplicate_count = len(results_df) - added_count\n",
    "    else:\n",
    "        combined = results_df.copy()\n",
    "        added_count = len(combined)\n",
    "        duplicate_count = 0\n",
    "\n",
    "    combined.to_csv(results_path, index=False)\n",
    "    print(f\" Results saved: {added_count} new rows  {results_path} (Duplicates skipped: {duplicate_count})\")\n",
    "\n",
    "    # Always overwrite failures\n",
    "    failures_df.to_csv(failures_path, index=False)\n",
    "    print(f\" Failures overwritten: {len(failures_df)}  {failures_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Idempotent Storage System - Smart Deduplication\n",
    "\n",
    "**Advanced save logic** with **idempotent append capabilities** and **composite key deduplication**:\n",
    "\n",
    "### Core Storage Strategy\n",
    "**Purpose**: Safely merge new recipient data with existing results while preventing duplicates\n",
    "\n",
    "### Deduplication Logic\n",
    "**Composite Key System**: \n",
    "- **Unique Keys**: `(fy, quarter, budget_function_code, budget_subfunction_code, federal_account_code, recipient_id)`\n",
    "- **Conflict Resolution**: New records replace existing records with same composite key\n",
    "- **Data Integrity**: Ensures each recipient appears only once per federal account context\n",
    "\n",
    "### File Management Process\n",
    "1. **Filename Derivation**: Extracts FY_Q pattern from input filename  \n",
    "2. **Existing File Handling**: Safely reads existing `recipients_{FY_Q}.csv` files\n",
    "3. **Corruption Protection**: Handles empty/corrupted existing files gracefully\n",
    "4. **Smart Merge**: Combines new and existing data with deduplication\n",
    "5. **Atomic Write**: Saves updated results to prevent partial file corruption\n",
    "\n",
    "### Failure File Strategy\n",
    "**Always Overwrites**: `failures_{FY_Q}.csv` files are completely replaced each run\n",
    "- **Current State**: Only shows failures from the most recent execution\n",
    "- **Historical Context**: Previous failure data is not preserved\n",
    "- **Clean Slate**: Each execution starts with fresh failure tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDA7WKX_7Hvx"
   },
   "outputs": [],
   "source": [
    "def run_initial_federal_account_processing(input_folder, output_folder, max_workers=50, start_fy=None):\n",
    "    \"\"\"\n",
    "    Processes all federal_accounts_*.csv files.\n",
    "    - Always writes recipients_{FY_Q}.csv (even if empty)\n",
    "    - Writes failures_{FY_Q}.csv ONLY if there are failures (deletes old one if it exists)\n",
    "    - Optional: start_fy to filter files by fiscal year\n",
    "    \"\"\"\n",
    "    for file in os.listdir(input_folder):\n",
    "        if not (file.endswith(\".csv\") and file.startswith(\"federal_accounts\")):\n",
    "            continue\n",
    "\n",
    "        # Optional FY filter from filename: federal_accounts_FY2024_Q1.csv\n",
    "        if start_fy is not None:\n",
    "            try:\n",
    "                fy = int(file.split(\"_FY\")[1].split(\"_Q\")[0])\n",
    "                if fy < start_fy:\n",
    "                    continue\n",
    "            except Exception:\n",
    "                print(f\" Skipped (cannot parse FY): {file}\")\n",
    "                continue\n",
    "\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        print(f\" Starting initial load for: {file_path}\")\n",
    "\n",
    "        df = read_and_filter_csv(file_path)\n",
    "        if df.empty:\n",
    "            print(f\" Skipped (no data): {file_path}\")\n",
    "            continue\n",
    "\n",
    "        results_df, failures_df = fetch_all_recipients(df, max_workers=max_workers)\n",
    "\n",
    "        base_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "        year_quarter = base_name.replace(\"federal_accounts_\", \"\")\n",
    "        results_path = os.path.join(output_folder, f\"recipients_{year_quarter}.csv\")\n",
    "        failures_path = os.path.join(output_folder, f\"failures_{year_quarter}.csv\")\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        #  Always save results\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\" Saved: {len(results_df)}  {results_path}\")\n",
    "\n",
    "        #  Only save failures if any; delete stale failures if none\n",
    "        if failures_df is not None and not failures_df.empty:\n",
    "            failures_df.to_csv(failures_path, index=False)\n",
    "            print(f\" Failures: {len(failures_df)}  {failures_path}\")\n",
    "        else:\n",
    "            if os.path.exists(failures_path):\n",
    "                os.remove(failures_path)\n",
    "                print(f\" Removed stale failures file: {failures_path}\")\n",
    "            print(f\" No failures for {year_quarter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initial Run - Complete Folder Processing\n",
    "\n",
    "**Primary data collection pipeline** that processes **all federal_accounts_*.csv files** in a folder:\n",
    "\n",
    "### Folder Processing Strategy\n",
    "**File Discovery Pattern**: Scans input folder for `federal_accounts*.csv` files\n",
    "- **Automatic Detection**: Finds all federal account files regardless of naming variations\n",
    "- **FY Filtering**: Optional `start_fy` parameter to skip older fiscal years\n",
    "- **Comprehensive Coverage**: Processes every discovered federal account file\n",
    "\n",
    "### Per-File Processing Flow\n",
    "1. **File Validation**: Confirms CSV format and federal_accounts prefix\n",
    "2. **FY Extraction**: Parses fiscal year from filename for optional filtering\n",
    "3. **Data Loading**: Reads federal account CSV using defensive reading\n",
    "4. **Parallel Collection**: Launches 50-worker ThreadPoolExecutor for recipient collection\n",
    "5. **Results Storage**: Saves recipients and failures with period-scoped naming\n",
    "\n",
    "### Output File Management\n",
    "**Always Created**: `recipients_{FY_Q}.csv` files (even if empty)\n",
    "- **Consistent Output**: Ensures every input file produces a corresponding recipient file\n",
    "- **Empty Handling**: Creates empty CSV with proper headers if no recipients found\n",
    "\n",
    "**Conditionally Created**: `failures_{FY_Q}.csv` files\n",
    "- **Only When Needed**: Created only if failures occur during collection\n",
    "- **Cleanup Logic**: Deletes existing failure files if no new failures occur\n",
    "- **Period Isolation**: Each fiscal year/quarter has separate failure tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbEw_085-rGy"
   },
   "outputs": [],
   "source": [
    "def run_failure_retry_from_folder(failure_folder, output_folder, max_workers=50):\n",
    "    \"\"\"\n",
    "    Retries all failures_*.csv in a folder.\n",
    "    - Deletes & skips empty/corrupt failure files BEFORE reading\n",
    "    - Appends new successful results to recipients_{FY_Q}.csv (via save_recipient_results)\n",
    "    - Overwrites failures_{FY_Q}.csv with new failures\n",
    "    - If new failures are empty, deletes failures_{FY_Q}.csv\n",
    "    \"\"\"\n",
    "    for file in os.listdir(failure_folder):\n",
    "        if not (file.endswith(\".csv\") and file.startswith(\"failures_\")):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(failure_folder, file)\n",
    "        print(f\" Retrying failures from: {file_path}\")\n",
    "\n",
    "        #  Delete 0-byte files up front\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            os.remove(file_path)\n",
    "            print(f\" Deleted empty failure file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Try reading safely\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            os.remove(file_path)\n",
    "            print(f\" Deleted corrupt failure file (EmptyDataError): {file_path}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\" Skipped (read error: {e}): {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Delete files that load but have no usable rows/columns\n",
    "        if df.empty or df.columns.size == 0:\n",
    "            os.remove(file_path)\n",
    "            print(f\" Deleted invalid failure file (no rows/cols): {file_path}\")\n",
    "            continue\n",
    "\n",
    "        #  Retry valid failures\n",
    "        results_df, failures_df = fetch_all_recipients(df, max_workers=max_workers)\n",
    "        save_recipient_results(results_df, failures_df, file_path, output_folder)\n",
    "\n",
    "        # If the fresh failures are empty, remove the just-written failures file\n",
    "        fyq = file.replace(\"failures_\", \"\").replace(\".csv\", \"\")\n",
    "        failures_out_path = os.path.join(output_folder, f\"failures_{fyq}.csv\")\n",
    "        if failures_df is None or failures_df.empty:\n",
    "            if os.path.exists(failures_out_path):\n",
    "                os.remove(failures_out_path)\n",
    "                print(f\" No remaining failures  deleted: {failures_out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Intelligent Retry System - Failure Recovery\n",
    "\n",
    "**Targeted retry mechanism** for failed recipient collection attempts with **defensive file handling**:\n",
    "\n",
    "### Retry Processing Strategy\n",
    "**File Discovery**: Scans folder for `failures_*.csv` files from previous runs\n",
    "- **Period-Scoped**: Each fiscal year/quarter has separate failure file\n",
    "- **Selective Processing**: Only retries previously failed federal account records\n",
    "- **Incremental Recovery**: Appends successful retries to existing recipient files\n",
    "\n",
    "### Defensive File Management\n",
    "**Pre-Processing Cleanup:**\n",
    "1. **Zero-Byte Detection**: Automatically deletes empty failure files before processing\n",
    "2. **Corruption Handling**: Safely handles corrupted CSV files with try/catch logic\n",
    "3. **File Validation**: Confirms readable CSV format before retry attempts\n",
    "4. **Pipeline Protection**: Prevents stuck pipelines from corrupt failure files\n",
    "\n",
    "### Retry Execution Flow\n",
    "1. **Failure File Validation**: Checks file size and readability\n",
    "2. **Failed Record Loading**: Reads federal account records that previously failed\n",
    "3. **Parallel Retry**: Uses ThreadPoolExecutor to re-attempt API calls\n",
    "4. **Incremental Append**: Uses `save_recipient_results()` for idempotent merging\n",
    "5. **Success Integration**: Merges retry successes with existing recipient data\n",
    "\n",
    "### Post-Retry Cleanup\n",
    "**Failure File Management:**\n",
    "- **New Failures**: Overwrites failure file if retry attempts still fail\n",
    "- **Complete Success**: Deletes failure file if all retries succeed\n",
    "- **Clean State**: Ensures only current failures are tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2lWlfCbGB1l"
   },
   "outputs": [],
   "source": [
    "def process_single_federal_account_file(file_path, output_folder, max_workers=50):\n",
    "    \"\"\"\n",
    "    Processes one federal_accounts_*.csv file and saves results and failures.\n",
    "    Overwrites both result and failure CSVs.\n",
    "    \"\"\"\n",
    "    print(f\" Processing single file: {file_path}\")\n",
    "    df = read_and_filter_csv(file_path)\n",
    "    if df.empty:\n",
    "        print(\" Skipped: No rows with obligated_amount > 0\")\n",
    "        return\n",
    "    results_df, failures_df = fetch_all_recipients(df, max_workers=max_workers)\n",
    "\n",
    "    # Reuse existing saving logic\n",
    "    save_recipient_results(results_df, failures_df, file_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Single-File Processing Helper\n",
    "\n",
    "**Streamlined processing** for individual federal account files:\n",
    "\n",
    "### Purpose & Use Cases\n",
    "**Single File Focus**: Processes one `federal_accounts_*.csv` file at a time\n",
    "- **Development/Testing**: Perfect for testing pipeline on individual files\n",
    "- **Selective Processing**: Process specific fiscal year/quarter combinations\n",
    "- **Debug/Analysis**: Isolate processing for troubleshooting specific periods\n",
    "\n",
    "### Processing Flow\n",
    "1. **File Loading**: Uses `read_and_filter_csv()` for defensive CSV reading\n",
    "2. **Data Validation**: Checks for empty DataFrame after loading\n",
    "3. **Parallel Collection**: Launches ThreadPoolExecutor for recipient data collection\n",
    "4. **Result Storage**: Uses `save_recipient_results()` for consistent output formatting\n",
    "\n",
    "### Output Behavior\n",
    "**Reuses Standard Logic**: Leverages existing `save_recipient_results()` function\n",
    "- **Idempotent Append**: Merges with existing recipient files if present\n",
    "- **Deduplication**: Applies composite key deduplication logic\n",
    "- **Failure Tracking**: Creates/updates failure files as needed\n",
    "\n",
    "**Consistency**: Produces identical output format as folder processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIfJAzzEGBki"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frtddIJsDPtT"
   },
   "outputs": [],
   "source": [
    "def main_controller(\n",
    "    mode,\n",
    "    input_folder=None,\n",
    "    output_folder=None,\n",
    "    single_file_path=None,\n",
    "    max_workers=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified entry point to:\n",
    "    - Run full initial load: mode='initial'\n",
    "    - Retry from failures: mode='retry'\n",
    "    - Run one specific file: mode='single'\n",
    "\n",
    "    Args:\n",
    "        mode (str): 'initial', 'retry', or 'single'\n",
    "        input_folder (str): Path to folder with input federal_accounts_*.csv files\n",
    "        output_folder (str): Path where recipient_* and failures_* files are stored\n",
    "        single_file_path (str): Path to one federal_accounts_*.csv file for single mode\n",
    "        max_workers (int): Thread pool size\n",
    "    \"\"\"\n",
    "    assert mode in {\"initial\", \"retry\", \"single\"}, \" Invalid mode. Choose: 'initial', 'retry', or 'single'\"\n",
    "\n",
    "    if mode == \"initial\":\n",
    "        if not input_folder or not output_folder:\n",
    "            raise ValueError(\" Please provide both input_folder and output_folder for initial mode.\")\n",
    "        run_initial_federal_account_processing(input_folder, output_folder, max_workers=max_workers)\n",
    "\n",
    "    elif mode == \"retry\":\n",
    "        if not input_folder or not output_folder:\n",
    "            raise ValueError(\" Please provide both input_folder and output_folder for retry mode.\")\n",
    "        run_failure_retry_from_folder(input_folder, output_folder, max_workers=max_workers)\n",
    "\n",
    "    elif mode == \"single\":\n",
    "        if not single_file_path or not output_folder:\n",
    "            raise ValueError(\" Please provide both single_file_path and output_folder for single mode.\")\n",
    "        process_single_federal_account_file(single_file_path, output_folder, max_workers=max_workers)\n",
    "\n",
    "    print(\" Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Master Controller - Unified Entry Point\n",
    "\n",
    "**Comprehensive orchestration system** with **three distinct processing modes**:\n",
    "\n",
    "### Controller Architecture\n",
    "**Single Entry Point**: Unified interface for all recipient collection operations\n",
    "- **Mode-Based Routing**: Intelligent dispatch based on processing requirements\n",
    "- **Parameter Validation**: Ensures required parameters for each mode\n",
    "- **Consistent Interface**: Standardized function signature across all modes\n",
    "\n",
    "### Processing Modes\n",
    "\n",
    "#### 1. **\"initial\"** Mode - Complete Data Harvest\n",
    "```python\n",
    "main_controller(\"initial\", \n",
    "    input_folder=\"/USASpendingResults\", \n",
    "    output_folder=\"/recipients\")\n",
    "```\n",
    "- **Purpose**: Processes ALL `federal_accounts_*.csv` files in source folder\n",
    "- **Input**: Federal account files from previous pipeline stages\n",
    "- **Output**: Complete recipient dataset with period-scoped organization\n",
    "\n",
    "#### 2. **\"retry\"** Mode - Failure Recovery\n",
    "```python\n",
    "main_controller(\"retry\", \n",
    "    input_folder=\"/recipients\", \n",
    "    output_folder=\"/recipients\")\n",
    "```\n",
    "- **Purpose**: Retries ALL `failures_*.csv` files in target folder\n",
    "- **Strategy**: Selective re-processing of previously failed federal accounts\n",
    "- **Integration**: Incrementally appends successful retries to existing recipient files\n",
    "\n",
    "#### 3. **\"single\"** Mode - Individual File Processing\n",
    "```python\n",
    "main_controller(\"single\", \n",
    "    single_file_path=\"/path/to/federal_accounts_FY2024_Q1.csv\", \n",
    "    output_folder=\"/recipients\")\n",
    "```\n",
    "- **Purpose**: Process one specific federal account file\n",
    "- **Use Cases**: Testing, debugging, selective processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11432,
     "status": "error",
     "timestamp": 1755713700777,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "38032EAMF82u",
    "outputId": "afbb9513-1b6f-4dbe-efe3-75b80330538b"
   },
   "outputs": [],
   "source": [
    "main_controller(\n",
    "    mode=\"initial\",\n",
    "    input_folder=\"/content/drive/MyDrive/USASpendingResults\",\n",
    "    output_folder=\"/content/drive/MyDrive/USASpendingResults/recipients\",\n",
    "    max_workers=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Execution Example - Initial Data Collection\n",
    "\n",
    "**Primary data collection run** processing all federal account files:\n",
    "\n",
    "### Configuration Details\n",
    "- **Mode**: `\"initial\"` - Complete folder processing\n",
    "- **Input Folder**: `/content/drive/MyDrive/USASpendingResults`\n",
    "  - Contains all `federal_accounts_*.csv` files from previous pipeline stages\n",
    "- **Output Folder**: `/content/drive/MyDrive/USASpendingResults/recipients`\n",
    "  - Will contain `recipients_{FY_Q}.csv` and `failures_{FY_Q}.csv` files\n",
    "- **Workers**: 20 concurrent threads (balanced performance for Colab)\n",
    "\n",
    "### Expected Process Flow\n",
    "1. **File Discovery**: Scans input folder for all `federal_accounts_*.csv` files\n",
    "2. **Parallel Processing**: 20-thread pool processes each federal account file\n",
    "3. **API Collection**: Calls USASpending.gov API with `type=\"recipient\"` filters\n",
    "4. **Data Organization**: Creates period-scoped recipient files\n",
    "5. **Failure Tracking**: Logs any API failures for retry processing\n",
    "\n",
    "### Typical Results\n",
    "- **Files Created**: Multiple `recipients_FY{YYYY}_Q{Q}.csv` files\n",
    "- **Failure Files**: `failures_FY{YYYY}_Q{Q}.csv` files (if failures occur)\n",
    "- **Data Scale**: Thousands of recipient records per federal account file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1308797,
     "status": "ok",
     "timestamp": 1755740670664,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "m_6CpwxNGSJa",
    "outputId": "46ac48b5-6b17-44d9-90ba-c5874c2d2956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q1.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2019_Q1.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q1.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q2.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2019_Q2.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q2.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q3.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2019_Q3.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q3.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q4.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2019_Q4.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2019_Q4.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q1.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2020_Q1.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 2  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q1.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q2.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2020_Q2.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 2  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q2.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q3.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2020_Q3.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 5  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q3.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q4.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2020_Q4.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 6  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2020_Q4.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2021_Q2.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2021_Q2.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2021_Q2.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2021_Q3.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2021_Q3.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2021_Q3.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2021_Q4.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2021_Q4.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 2  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2021_Q4.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q1.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2022_Q1.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q1.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q2.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2022_Q2.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q2.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q3.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2022_Q3.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q3.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q4.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2022_Q4.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 3  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2022_Q4.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2023_Q3.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2023_Q3.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2023_Q3.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2023_Q4.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2023_Q4.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2023_Q4.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2024_Q3.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2024_Q3.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 1  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2024_Q3.csv\n",
      " Retrying failures from: /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2024_Q4.csv\n",
      " Results saved: 0 new rows  /content/drive/MyDrive/USASpendingResults/recipients/recipients_FY2024_Q4.csv (Duplicates skipped: 0)\n",
      " Failures overwritten: 2  /content/drive/MyDrive/USASpendingResults/recipients/failures_FY2024_Q4.csv\n",
      " Done.\n"
     ]
    }
   ],
   "source": [
    "main_controller(\n",
    "    mode=\"retry\",\n",
    "    input_folder=\"/content/drive/MyDrive/USASpendingResults/recipients\",\n",
    "    output_folder=\"/content/drive/MyDrive/USASpendingResults/recipients\",\n",
    "    max_workers=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Execution Example - Retry Failed Collections\n",
    "\n",
    "**Targeted retry processing** for previously failed recipient collection attempts:\n",
    "\n",
    "### Configuration Details  \n",
    "- **Mode**: `\"retry\"` - Failure recovery processing\n",
    "- **Input Folder**: `/content/drive/MyDrive/USASpendingResults/recipients`\n",
    "  - Same as output folder - looks for `failures_*.csv` files created during initial run\n",
    "- **Output Folder**: `/content/drive/MyDrive/USASpendingResults/recipients`\n",
    "  - Updates existing recipient files with retry successes\n",
    "- **Workers**: 20 concurrent threads (conservative retry approach)\n",
    "\n",
    "### Retry Process Flow\n",
    "1. **Failure Discovery**: Scans recipient folder for `failures_*.csv` files\n",
    "2. **File Validation**: Deletes empty/corrupted failure files automatically\n",
    "3. **Selective Retry**: Only processes federal accounts that failed initially  \n",
    "4. **Incremental Append**: Merges successful retries with existing recipient data\n",
    "5. **Cleanup Logic**: Deletes failure files if no remaining failures\n",
    "\n",
    "### Expected Outcomes\n",
    "- **Success Recovery**: Previously failed federal accounts may now succeed\n",
    "- **Data Completeness**: Improved recipient data coverage\n",
    "- **Clean State**: Remaining failures represent legitimate data gaps\n",
    "- **File Management**: Automatic cleanup of resolved failure files\n",
    "\n",
    "**Best Practice**: Run retry after initial collection to maximize data completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMmkOXKnvY1g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}