{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Geography-Based Federal Spending Data Collection\n",
    "\n",
    "##  Overview\n",
    "\n",
    "**Advanced multi-layered geography data collection** from USASpending.gov API with enterprise-grade reliability features:\n",
    "\n",
    "###  Core Purpose\n",
    "- **Geographic Spending Analysis**: Pull spending data by country/state/county/district from USAspending\n",
    "- **Fiscal Year & Quarter Granularity**: Comprehensive temporal coverage across multiple fiscal years\n",
    "- **High-Performance Collection**: Optimized for large-scale data retrieval with connection pooling\n",
    "- **Production-Ready Reliability**: Built-in failover, retry logic, and error recovery\n",
    "\n",
    "###  Architecture Highlights\n",
    "- ** Connection Pooling**: Fast keep-alive connections for optimal throughput\n",
    "- ** Escalating Failover**: Shared session  fresh session  raw request progression\n",
    "- ** Smart Concurrency**: Per-layer worker sizing to prevent resource starvation\n",
    "- ** Robust Error Handling**: Backoff + jitter on network exceptions\n",
    "- ** Failure Tracking**: Per-layer failure files for selective retry runs\n",
    "- ** Incremental Output**: Per-year merged outputs for efficient data management\n",
    "\n",
    "###  Geographic Layers\n",
    "1. **Country**: International spending data\n",
    "2. **State**: US state-level spending breakdown  \n",
    "3. **County**: County-level granular analysis (with FIPS mapping)\n",
    "4. **District**: Congressional district spending patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1757443509822,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "bSjd_r3mTNsj"
   },
   "outputs": [],
   "source": [
    "import os, time, json, random, requests, pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from http.client import RemoteDisconnected\n",
    "from requests.exceptions import ConnectionError, ReadTimeout, ChunkedEncodingError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dependencies & Configuration Setup\n",
    "\n",
    "**Essential imports and global configuration** for high-performance geographic data collection:\n",
    "\n",
    "### Core Libraries\n",
    "- **`requests`**: HTTP client with session management and connection pooling\n",
    "- **`pandas`**: Data processing and DataFrame operations for normalized output\n",
    "- **`concurrent.futures`**: ThreadPoolExecutor for parallel API requests\n",
    "- **`time`**: Exponential backoff and jitter timing control\n",
    "- **`json`**: API response parsing and payload construction\n",
    "\n",
    "### Key Configuration Variables\n",
    "- **`MAX_WORKERS`**: Default thread pool size for concurrent requests\n",
    "- **`MAX_ATTEMPTS_EXC`**: Maximum retry attempts per failed request\n",
    "- **`TIMEOUT_S`**: Request timeout in seconds (connect + read)\n",
    "- **`URL`**: USASpending API endpoint (`/api/v2/spending_by_geography/`)\n",
    "- **`SESSION`**: Shared requests session with connection pooling\n",
    "- **`PAUSE`**: Small sleep between attempts to prevent API overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1757443510456,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "mT9AptajTRZi",
    "outputId": "b505304a-e98b-4736-d86a-84597ddf1559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Per-Layer Worker Configuration\n",
    "\n",
    "**Optimized concurrency settings** to prevent resource starvation and ensure fair processing:\n",
    "\n",
    "### Worker Sizing Strategy\n",
    "- **Country Layer**: 4 workers (lightweight, fast responses)\n",
    "- **State Layer**: 4 workers (moderate data volume)\n",
    "- **County Layer**: 4 workers (high volume, FIPS processing)\n",
    "- **District Layer**: 4 workers (congressional district complexity)\n",
    "\n",
    "### Why Per-Layer Sizing?\n",
    "- **Resource Management**: Prevents heavy layers from monopolizing connections\n",
    "- **API Rate Limiting**: Distributes request load evenly across geographic types\n",
    "- **Memory Efficiency**: Balances concurrent requests with system resources\n",
    "- **Scalability**: Easy to tune individual layers based on performance metrics\n",
    "\n",
    "### Dynamic Worker Selection\n",
    "The `get_worker_count(layer)` function provides fallback to `MAX_WORKERS` for undefined layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1757443926963,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "iQOlztJVy856"
   },
   "outputs": [],
   "source": [
    "# Put near your CONFIG\n",
    "LAYER_WORKERS = {\"country\": 4, \"state\": 4, \"county\": 4, \"district\": 4}\n",
    "\n",
    "def get_worker_count(layer: str) -> int:\n",
    "    return LAYER_WORKERS.get(layer, MAX_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Session Setup & Connection Pooling\n",
    "\n",
    "**Enterprise-grade HTTP session management** for maximum throughput and reliability:\n",
    "\n",
    "### Connection Pool Strategy\n",
    "- **Keep-Alive Connections**: Reuses TCP connections to reduce handshake overhead\n",
    "- **Pool Sizing**: `MAX_WORKERS + 8` connections to handle concurrent requests\n",
    "- **Pool Blocking**: Threads wait for available connections instead of failing\n",
    "- **HTTPS Adapter**: Optimized for USASpending.gov's SSL endpoints\n",
    "\n",
    "### Retry Policy Design\n",
    "- **No HTTP Status Retries**: Application-level retry logic handles 4xx/5xx responses\n",
    "- **Transport Error Focus**: Only retries on network-level failures\n",
    "- **Connection Header**: Forces keep-alive for persistent connections\n",
    "\n",
    "### Session Benefits\n",
    "- **Performance**: 50-80% faster than individual requests\n",
    "- **Resource Efficiency**: Shared connection pool across all threads\n",
    "- **Reliability**: Built-in connection lifecycle management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1757443547428,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "jpLH3HnLzEEZ"
   },
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry  # only for connection-level backoff (not HTTP codes)\n",
    "\n",
    "def setup_session(pool_maxsize=None):\n",
    "    if pool_maxsize is None:\n",
    "        pool_maxsize = MAX_WORKERS + 8\n",
    "    s = requests.Session()\n",
    "    adapter = HTTPAdapter(\n",
    "        pool_connections=pool_maxsize,\n",
    "        pool_maxsize=pool_maxsize,\n",
    "        max_retries=Retry(total=0, connect=0, read=0, redirect=0, status=0),  # no HTTP status retries\n",
    "        pool_block=True,  # block instead of dropping connections\n",
    "    )\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.headers.update({\"Connection\": \"keep-alive\"})\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  API Payload Construction\n",
    "\n",
    "**Dynamic payload generation** for USASpending.gov geography endpoints:\n",
    "\n",
    "### Payload Structure\n",
    "- **Time Filters**: Fiscal year and quarter specification\n",
    "- **Geographic Layer**: Target geographic granularity (country/state/county/district)\n",
    "- **Scope & Aggregation**: Controls data grouping and summarization level\n",
    "- **Additional Filters**: Optional agency, account, or award type constraints\n",
    "\n",
    "### Geographic Layer Types\n",
    "- **`country`**: International spending analysis\n",
    "- **`state`**: US state-level breakdowns  \n",
    "- **`county`**: County-level with FIPS code integration\n",
    "- **`district`**: Congressional district spending patterns\n",
    "\n",
    "### API Endpoint Strategy\n",
    "- **Primary**: `/api/v2/search/spending_by_geography/` (newer search API)\n",
    "- **Fallback**: `/api/v2/spending_by_geography/` (legacy endpoint)\n",
    "- **Flexibility**: Supports both POST and GET request patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1757443557560,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "dQTm6K_KzGDA"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from http.client import RemoteDisconnected\n",
    "from requests.exceptions import ConnectionError, ReadTimeout, ChunkedEncodingError\n",
    "\n",
    "RETRY_EXC = (RemoteDisconnected, ConnectionError, ReadTimeout, ChunkedEncodingError)\n",
    "\n",
    "def fetch_one(layer:str, fy:int, q:int, attempts=MAX_ATTEMPTS_EXC):\n",
    "    \"\"\"Fetch one layer/FY/Q with escalating fallbacks.\"\"\"\n",
    "    for attempt in range(1, attempts+1):\n",
    "        try:\n",
    "            # Try with shared session first\n",
    "            r = SESSION.post(\n",
    "                URL,\n",
    "                json=payload_for(layer, fy, q),\n",
    "                timeout=TIMEOUT_S,\n",
    "            )\n",
    "            try:\n",
    "                data = r.json()\n",
    "            except ValueError:\n",
    "                data = {\"raw_text\": r.text[:400]}\n",
    "            if r.status_code != 200:\n",
    "                raise RuntimeError(f\"HTTP {r.status_code} {str(data)[:300]}\")\n",
    "            df = pd.DataFrame(data.get(\"results\", []))\n",
    "            if df.empty:\n",
    "                df = pd.DataFrame(columns=[\"code\",\"name\",\"amount\",\"population\",\"fy\",\"quarter\",\"geo_layer\"])\n",
    "            else:\n",
    "                df = df.rename(columns={\"shape_code\":\"code\",\"display_name\":\"name\",\"aggregated_amount\":\"amount\"})\n",
    "                keep = [\"code\",\"name\",\"amount\",\"population\"]\n",
    "                df = df[[c for c in keep if c in df.columns]]\n",
    "                df[\"amount\"] = pd.to_numeric(df.get(\"amount\"), errors=\"coerce\")\n",
    "                df[\"fy\"], df[\"quarter\"], df[\"geo_layer\"] = fy, q, layer\n",
    "                if layer == \"county\" and \"code\" in df.columns:\n",
    "                    df[\"state_code\"] = df[\"code\"].astype(str).str[:2]\n",
    "                    df[\"state_name\"] = df[\"state_code\"].map(FIPS_TO_STATE)\n",
    "            return df\n",
    "\n",
    "        except RETRY_EXC as e:\n",
    "            # escalate strategy by attempt\n",
    "            if attempt == 2:\n",
    "                # new ephemeral session\n",
    "                with setup_session(pool_maxsize=8) as s2:\n",
    "                    try:\n",
    "                        r = s2.post(URL, json=payload_for(layer, fy, q), timeout=TIMEOUT_S)\n",
    "                        data = r.json() if r.headers.get(\"content-type\",\"\").startswith(\"application/json\") else {\"raw_text\": r.text[:400]}\n",
    "                        if r.status_code == 200:\n",
    "                            df = pd.DataFrame(data.get(\"results\", []))\n",
    "                            if df.empty:\n",
    "                                df = pd.DataFrame(columns=[\"code\",\"name\",\"amount\",\"population\",\"fy\",\"quarter\",\"geo_layer\"])\n",
    "                            else:\n",
    "                                df = df.rename(columns={\"shape_code\":\"code\",\"display_name\":\"name\",\"aggregated_amount\":\"amount\"})\n",
    "                                keep = [\"code\",\"name\",\"amount\",\"population\"]\n",
    "                                df = df[[c for c in keep if c in df.columns]]\n",
    "                                df[\"amount\"] = pd.to_numeric(df.get(\"amount\"), errors=\"coerce\")\n",
    "                                df[\"fy\"], df[\"quarter\"], df[\"geo_layer\"] = fy, q, layer\n",
    "                                if layer == \"county\" and \"code\" in df.columns:\n",
    "                                    df[\"state_code\"] = df[\"code\"].astype(str).str[:2]\n",
    "                                    df[\"state_name\"] = df[\"state_code\"].map(FIPS_TO_STATE)\n",
    "                            return df\n",
    "                    except RETRY_EXC:\n",
    "                        pass\n",
    "            if attempt >= 3:\n",
    "                # raw request + force TCP close (some backends prefer this)\n",
    "                r = requests.post(\n",
    "                    URL,\n",
    "                    json=payload_for(layer, fy, q),\n",
    "                    timeout=TIMEOUT_S,\n",
    "                    headers={\"Connection\": \"close\"}\n",
    "                )\n",
    "                try:\n",
    "                    data = r.json()\n",
    "                except ValueError:\n",
    "                    data = {\"raw_text\": r.text[:400]}\n",
    "                if r.status_code == 200:\n",
    "                    df = pd.DataFrame(data.get(\"results\", []))\n",
    "                    if df.empty:\n",
    "                        df = pd.DataFrame(columns=[\"code\",\"name\",\"amount\",\"population\",\"fy\",\"quarter\",\"geo_layer\"])\n",
    "                    else:\n",
    "                        df = df.rename(columns={\"shape_code\":\"code\",\"display_name\":\"name\",\"aggregated_amount\":\"amount\"})\n",
    "                        keep = [\"code\",\"name\",\"amount\",\"population\"]\n",
    "                        df = df[[c for c in keep if c in df.columns]]\n",
    "                        df[\"amount\"] = pd.to_numeric(df.get(\"amount\"), errors=\"coerce\")\n",
    "                        df[\"fy\"], df[\"quarter\"], df[\"geo_layer\"] = fy, q, layer\n",
    "                        if layer == \"county\" and \"code\" in df.columns:\n",
    "                            df[\"state_code\"] = df[\"code\"].astype(str).str[:2]\n",
    "                            df[\"state_name\"] = df[\"state_code\"].map(FIPS_TO_STATE)\n",
    "                    return df\n",
    "\n",
    "            if attempt == attempts:\n",
    "                raise\n",
    "            # backoff + jitter\n",
    "            backoff = (0.4 * (2 ** (attempt - 1))) + random.uniform(0, 0.4)\n",
    "            print(f\" retry {layer} FY{fy} Q{q} ({attempt}/{attempts}) after {type(e).__name__}: {e}\")\n",
    "            time.sleep(backoff)\n",
    "\n",
    "        finally:\n",
    "            time.sleep(PAUSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Core Fetch Function: Escalating Failover Strategy\n",
    "\n",
    "**The heart of the collection system** with intelligent retry and failover mechanisms:\n",
    "\n",
    "### Three-Tier Failover Approach\n",
    "\n",
    "#### **Attempt 1: Shared Session (Fastest)**\n",
    "- Uses the global pooled `SESSION` with keep-alive connections\n",
    "- Optimized for maximum throughput with connection reuse\n",
    "- Handles 90%+ of requests under normal conditions\n",
    "\n",
    "#### **Attempt 2: Fresh Session (Clean Slate)**\n",
    "- Creates ephemeral session with `setup_session(pool_maxsize=8)`\n",
    "- Bypasses potential connection pool corruption issues\n",
    "- Isolates problematic requests from the shared pool\n",
    "\n",
    "#### **Attempt 3: Raw Request (Connection: Close)**\n",
    "- Forces new TCP connection with `Connection: close` header\n",
    "- Breaks suspected keep-alive issues with backend servers\n",
    "- Last resort for persistent connection problems\n",
    "\n",
    "### Data Normalization Pipeline\n",
    "- **Field Standardization**: `shape_code`  `code`, `display_name`  `name`\n",
    "- **Schema Consistency**: Ensures `[\"code\",\"name\",\"amount\",\"population\",\"fy\",\"quarter\",\"geo_layer\"]`\n",
    "- **County Enhancement**: Adds `state_code` and `state_name` via FIPS lookup\n",
    "- **Empty Result Handling**: Returns proper schema even for zero-result queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1757443578999,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "oW2mvT51zIhL"
   },
   "outputs": [],
   "source": [
    "def initial_run(start_fy:int, end_fy:int, layers:list[str]):\n",
    "    ensure_dirs()\n",
    "    init_failures_files(overwrite=True)\n",
    "    by_layer_year: dict[str, dict[int, list[pd.DataFrame]]] = {lyr:{} for lyr in layers}\n",
    "\n",
    "    for layer in layers:\n",
    "        tasks = [(layer, fy, q) for fy in range(start_fy, end_fy+1) for q in (1,2,3,4)]\n",
    "        workers = get_worker_count(layer)\n",
    "        print(f\"Submitting {len(tasks)} tasks for {layer} (initial) with max_workers={workers} \")\n",
    "        with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "            fut2task = {ex.submit(fetch_one, lyr, fy, q): (lyr, fy, q) for (lyr, fy, q) in tasks}\n",
    "            for fut in as_completed(fut2task):\n",
    "                _, fy, q = fut2task[fut]\n",
    "                try:\n",
    "                    dfq = fut.result()\n",
    "                    by_layer_year[layer].setdefault(fy, []).append(dfq)\n",
    "                    print(f\" {layer} FY{fy} Q{q}: {len(dfq)} rows\")\n",
    "                except Exception as e:\n",
    "                    print(f\" {layer} FY{fy} Q{q} failed: {e}\")\n",
    "                    append_failure(layer, fy, q, str(e))\n",
    "\n",
    "    # save per-year + all-years (same as before) ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exponential Backoff & Error Handling\n",
    "\n",
    "**Smart retry timing and robust error recovery** for production reliability:\n",
    "\n",
    "### Backoff Algorithm\n",
    "```python\n",
    "backoff = (0.4 * (2 ** (attempt - 1))) + random.uniform(0, 0.4)\n",
    "```\n",
    "\n",
    "### Timing Strategy\n",
    "- **Attempt 1**: No delay (immediate retry)\n",
    "- **Attempt 2**: ~0.4-0.8 seconds (base + jitter)\n",
    "- **Attempt 3**: ~0.8-1.2 seconds (exponential growth)\n",
    "- **Final Pause**: Small consistent `PAUSE` after each request\n",
    "\n",
    "### Targeted Exception Handling\n",
    "- **`RemoteDisconnected`**: Server closed connection unexpectedly\n",
    "- **`ConnectionError`**: Network connectivity issues\n",
    "- **`ReadTimeout`**: Server response timeout\n",
    "- **`ChunkedEncodingError`**: HTTP transfer encoding problems\n",
    "\n",
    "### Why This Approach?\n",
    "- **Jitter Prevention**: Random component prevents thundering herd\n",
    "- **Progressive Delays**: Gives struggling servers time to recover\n",
    "- **Transport Focus**: Only retries on network issues, not application errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1757444499494,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "HVXvVqWQzNwK"
   },
   "outputs": [],
   "source": [
    "def retry_run(layers: list[str]):\n",
    "    ensure_dirs()\n",
    "    for layer in layers:\n",
    "        # read current failures for this layer\n",
    "        f = read_failures(layer)\n",
    "        f = f[pd.to_numeric(f[\"fy\"], errors=\"coerce\").notna() & pd.to_numeric(f[\"quarter\"], errors=\"coerce\").notna()]\n",
    "        todo = sorted(list(set((layer, int(row.fy), int(row.quarter)) for _, row in f.iterrows())))\n",
    "        if not todo:\n",
    "            print(f\" No failures to retry for {layer}.\")\n",
    "            # also clear the file if it has stale rows\n",
    "            fp = failures_path(layer)\n",
    "            if os.path.exists(fp) and os.path.getsize(fp) > 0:\n",
    "                os.remove(fp)\n",
    "            continue\n",
    "\n",
    "        by_year: dict[int, list[pd.DataFrame]] = {}\n",
    "        new_failures: list[tuple[str,int,int,str]] = []  # collect only failures from this retry run\n",
    "\n",
    "        workers = get_worker_count(layer)\n",
    "        print(f\" Retrying {len(todo)} failed tasks for {layer} with max_workers={workers} \")\n",
    "        with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "            fut2task = {ex.submit(fetch_one, lyr, fy, q): (lyr, fy, q) for (lyr, fy, q) in todo}\n",
    "            for fut in as_completed(fut2task):\n",
    "                _, fy, q = fut2task[fut]\n",
    "                try:\n",
    "                    dfq = fut.result()\n",
    "                    by_year.setdefault(fy, []).append(dfq)\n",
    "                    print(f\" retry OK: {layer} FY{fy} Q{q} ({len(dfq)} rows)\")\n",
    "                except Exception as e:\n",
    "                    print(f\" retry failed: {layer} FY{fy} Q{q}: {e}\")\n",
    "                    new_failures.append((layer, fy, q, str(e)))  # don't append to file yet\n",
    "\n",
    "        # merge-append per year for successes\n",
    "        for fy, parts in by_year.items():\n",
    "            out, n = save_year_merge(layer, fy, parts)\n",
    "            print(f\" merged {layer} FY{fy}: {n} rows  {out}\")\n",
    "\n",
    "        # OVERWRITE the failures file to reflect only still-failing tasks\n",
    "        fp = failures_path(layer)\n",
    "        if new_failures:\n",
    "            df = pd.DataFrame(new_failures, columns=[\"layer\",\"fy\",\"quarter\",\"reason\"])\n",
    "            df.to_csv(fp, index=False)\n",
    "            print(f\" updated failures for {layer}: {len(df)} rows  {fp}\")\n",
    "        else:\n",
    "            if os.path.exists(fp):\n",
    "                os.remove(fp)\n",
    "            print(f\" no failures remain for {layer}; cleared {fp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initial Run Orchestration\n",
    "\n",
    "**Primary data collection workflow** with comprehensive task management:\n",
    "\n",
    "### Execution Flow\n",
    "1. **Directory Setup**: Creates output directories and initializes failure tracking\n",
    "2. **Task Generation**: Builds all (fiscal_year, quarter) combinations per layer\n",
    "3. **Parallel Execution**: Submits tasks to layer-specific thread pools\n",
    "4. **Success Collection**: Stores successful DataFrames by layer and fiscal year\n",
    "5. **Failure Tracking**: Logs failed tasks to per-layer CSV files for retry\n",
    "\n",
    "### Progress Monitoring\n",
    "- **Task Submission**: `\"Submitting 32 tasks for county (initial) with max_workers=4\"`\n",
    "- **Success Logging**: `\" county FY2021 Q3: 3143 rows\"`\n",
    "- **Failure Tracking**: `\" county FY2020 Q2 failed: HTTP 500 {...}\"`\n",
    "\n",
    "### Data Organization\n",
    "- **Per-Layer Collection**: `by_layer_year[layer][fy]` structure\n",
    "- **Year-Based Merging**: Combines quarterly data into annual files\n",
    "- **Incremental Output**: Enables partial collection recovery\n",
    "\n",
    "### Failure Recovery Preparation\n",
    "- **Granular Tracking**: Records exact (layer, FY, quarter) combinations that failed\n",
    "- **Retry Foundation**: Creates CSV files for selective re-execution\n",
    "- **State Preservation**: Maintains collection progress across interruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 118724,
     "status": "ok",
     "timestamp": 1757444621217,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "_lwHwj0xzQnP",
    "outputId": "c26ea4e2-6cd0-4c46-e480-3843ba62514d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting 68 tasks for country (initial) with max_workers=4 \n",
      " country FY2008 Q1: 197 rows\n",
      " country FY2008 Q3: 197 rows\n",
      " country FY2008 Q2: 195 rows\n",
      " country FY2008 Q4: 207 rows\n",
      " country FY2009 Q1: 196 rows\n",
      " country FY2009 Q3: 204 rows\n",
      " country FY2009 Q2: 203 rows\n",
      " country FY2009 Q4: 213 rows\n",
      " country FY2010 Q1: 203 rows\n",
      " country FY2010 Q2: 203 rows\n",
      " country FY2010 Q3: 206 rows\n",
      " country FY2010 Q4: 213 rows\n",
      " country FY2011 Q1: 200 rows\n",
      " country FY2011 Q3: 221 rows\n",
      " country FY2011 Q2: 209 rows\n",
      " country FY2011 Q4: 225 rows\n",
      " country FY2012 Q1: 222 rows\n",
      " country FY2012 Q2: 219 rows\n",
      " country FY2012 Q3: 220 rows\n",
      " country FY2013 Q2: 224 rows\n",
      " country FY2012 Q4: 224 rows\n",
      " country FY2013 Q1: 223 rows\n",
      " country FY2013 Q3: 221 rows\n",
      " country FY2013 Q4: 224 rows\n",
      " country FY2014 Q1: 217 rows\n",
      " country FY2014 Q3: 222 rows\n",
      " country FY2014 Q2: 221 rows\n",
      " country FY2015 Q1: 219 rows\n",
      " country FY2014 Q4: 223 rows\n",
      " country FY2015 Q2: 223 rows\n",
      " country FY2015 Q3: 226 rows\n",
      " country FY2016 Q1: 225 rows\n",
      " country FY2015 Q4: 224 rows\n",
      " country FY2016 Q2: 224 rows\n",
      " country FY2016 Q4: 228 rows\n",
      " country FY2017 Q1: 226 rows\n",
      " country FY2016 Q3: 222 rows\n",
      " country FY2017 Q2: 227 rows\n",
      " country FY2018 Q1: 224 rows\n",
      " country FY2017 Q4: 227 rows\n",
      " country FY2017 Q3: 228 rows\n",
      " country FY2018 Q2: 224 rows\n",
      " country FY2018 Q4: 230 rows\n",
      " country FY2018 Q3: 227 rows\n",
      " country FY2019 Q1: 223 rows\n",
      " country FY2019 Q4: 229 rows\n",
      " country FY2019 Q2: 224 rows\n",
      " country FY2020 Q1: 221 rows\n",
      " country FY2019 Q3: 226 rows\n",
      " country FY2020 Q2: 226 rows\n",
      " country FY2020 Q3: 224 rows\n",
      " country FY2020 Q4: 228 rows\n",
      " country FY2021 Q2: 225 rows\n",
      " country FY2021 Q1: 222 rows\n",
      " country FY2021 Q4: 228 rows\n",
      " country FY2021 Q3: 228 rows\n",
      " country FY2022 Q4: 228 rows\n",
      " country FY2022 Q1: 224 rows\n",
      " country FY2022 Q2: 226 rows\n",
      " country FY2023 Q1: 223 rows\n",
      " country FY2022 Q3: 227 rows\n",
      " country FY2023 Q3: 228 rows\n",
      " country FY2023 Q2: 224 rows\n",
      " country FY2023 Q4: 228 rows\n",
      " country FY2024 Q2: 228 rows\n",
      " country FY2024 Q3: 228 rows\n",
      " country FY2024 Q1: 225 rows\n",
      " country FY2024 Q4: 228 rows\n",
      "Submitting 68 tasks for state (initial) with max_workers=4 \n",
      " state FY2008 Q1: 57 rows\n",
      " state FY2008 Q2: 57 rows\n",
      " state FY2008 Q4: 57 rows\n",
      " state FY2008 Q3: 57 rows\n",
      " state FY2009 Q1: 57 rows\n",
      " state FY2009 Q3: 57 rows\n",
      " state FY2009 Q4: 57 rows\n",
      " state FY2009 Q2: 57 rows\n",
      " state FY2010 Q1: 57 rows\n",
      " state FY2010 Q2: 57 rows\n",
      " state FY2010 Q3: 57 rows\n",
      " state FY2010 Q4: 57 rows\n",
      " state FY2011 Q1: 57 rows\n",
      " state FY2011 Q2: 57 rows\n",
      " state FY2011 Q3: 57 rows\n",
      " state FY2012 Q1: 57 rows\n",
      " state FY2011 Q4: 57 rows\n",
      " state FY2012 Q2: 57 rows\n",
      " state FY2012 Q4: 57 rows\n",
      " state FY2013 Q1: 57 rows\n",
      " state FY2012 Q3: 57 rows\n",
      " state FY2013 Q3: 57 rows\n",
      " state FY2013 Q2: 57 rows\n",
      " state FY2014 Q1: 57 rows\n",
      " state FY2013 Q4: 57 rows\n",
      " state FY2014 Q3: 57 rows\n",
      " state FY2014 Q2: 57 rows\n",
      " state FY2014 Q4: 57 rows\n",
      " state FY2015 Q2: 57 rows\n",
      " state FY2015 Q4: 57 rows\n",
      " state FY2015 Q3: 57 rows\n",
      " state FY2016 Q1: 57 rows\n",
      " state FY2015 Q1: 57 rows\n",
      " state FY2016 Q3: 57 rows\n",
      " state FY2016 Q2: 57 rows\n",
      " state FY2017 Q1: 57 rows\n",
      " state FY2017 Q2: 57 rows\n",
      " state FY2017 Q3: 57 rows\n",
      " state FY2018 Q2: 57 rows\n",
      " state FY2016 Q4: 57 rows\n",
      " state FY2017 Q4: 57 rows\n",
      " state FY2018 Q1: 57 rows\n",
      " state FY2018 Q3: 57 rows\n",
      " state FY2018 Q4: 57 rows\n",
      " state FY2019 Q1: 57 rows\n",
      " state FY2019 Q2: 57 rows\n",
      " state FY2019 Q3: 57 rows\n",
      " state FY2019 Q4: 57 rows\n",
      " state FY2020 Q1: 57 rows\n",
      " state FY2020 Q2: 57 rows\n",
      " state FY2020 Q4: 57 rows\n",
      " state FY2020 Q3: 57 rows\n",
      " state FY2021 Q2: 57 rows\n",
      " state FY2021 Q4: 57 rows\n",
      " state FY2021 Q3: 57 rows\n",
      " state FY2021 Q1: 57 rows\n",
      " state FY2022 Q1: 57 rows\n",
      " state FY2022 Q2: 57 rows\n",
      " state FY2022 Q3: 57 rows\n",
      " state FY2022 Q4: 57 rows\n",
      " state FY2023 Q2: 57 rows\n",
      " state FY2023 Q1: 57 rows\n",
      " state FY2023 Q3: 57 rows\n",
      " state FY2024 Q1: 57 rows\n",
      " state FY2023 Q4: 57 rows\n",
      " state FY2024 Q3: 57 rows\n",
      " state FY2024 Q2: 57 rows\n",
      " state FY2024 Q4: 57 rows\n",
      "Submitting 68 tasks for county (initial) with max_workers=4 \n",
      " county FY2008 Q1: 3227 rows\n",
      " county FY2008 Q3: 3228 rows\n",
      " county FY2008 Q4: 3230 rows\n",
      " county FY2008 Q2: 3228 rows\n",
      " county FY2009 Q1: 3230 rows\n",
      " county FY2009 Q2: 3229 rows\n",
      " county FY2009 Q3: 3231 rows\n",
      " county FY2009 Q4: 3229 rows\n",
      " county FY2010 Q1: 3229 rows\n",
      " county FY2010 Q2: 3228 rows\n",
      " county FY2010 Q3: 3228 rows\n",
      " county FY2010 Q4: 3232 rows\n",
      " county FY2011 Q1: 3231 rows\n",
      " county FY2011 Q2: 3233 rows\n",
      " county FY2011 Q3: 3233 rows\n",
      " county FY2011 Q4: 3233 rows\n",
      " county FY2012 Q2: 3233 rows\n",
      " county FY2012 Q1: 3232 rows\n",
      " county FY2012 Q3: 3235 rows\n",
      " county FY2012 Q4: 3233 rows\n",
      " county FY2013 Q1: 3234 rows\n",
      " county FY2013 Q2: 3235 rows\n",
      " county FY2013 Q3: 3235 rows\n",
      " county FY2013 Q4: 3231 rows\n",
      " county FY2014 Q1: 3230 rows\n",
      " county FY2014 Q2: 3231 rows\n",
      " county FY2014 Q3: 3231 rows\n",
      " county FY2014 Q4: 3231 rows\n",
      " county FY2015 Q1: 3230 rows\n",
      " county FY2015 Q2: 3231 rows\n",
      " county FY2015 Q3: 3231 rows\n",
      " county FY2015 Q4: 3232 rows\n",
      " county FY2016 Q1: 3232 rows\n",
      " county FY2016 Q2: 3232 rows\n",
      " county FY2016 Q3: 3231 rows\n",
      " county FY2016 Q4: 3232 rows\n",
      " county FY2017 Q1: 3232 rows\n",
      " county FY2017 Q2: 3233 rows\n",
      " county FY2017 Q3: 3231 rows\n",
      " county FY2017 Q4: 3233 rows\n",
      " county FY2018 Q2: 3232 rows\n",
      " county FY2018 Q3: 3231 rows\n",
      " county FY2018 Q1: 3232 rows\n",
      " county FY2018 Q4: 3232 rows\n",
      " county FY2019 Q1: 3231 rows\n",
      " county FY2019 Q2: 3234 rows\n",
      " county FY2019 Q3: 3233 rows\n",
      " county FY2019 Q4: 3233 rows\n",
      " county FY2020 Q2: 3233 rows\n",
      " county FY2020 Q1: 3233 rows\n",
      " county FY2020 Q3: 3233 rows\n",
      " county FY2020 Q4: 3233 rows\n",
      " county FY2021 Q2: 3233 rows\n",
      " county FY2021 Q3: 3233 rows\n",
      " county FY2021 Q1: 3233 rows\n",
      " county FY2021 Q4: 3233 rows\n",
      " county FY2022 Q2: 3233 rows\n",
      " county FY2022 Q1: 3233 rows\n",
      " county FY2022 Q4: 3233 rows\n",
      " county FY2022 Q3: 3233 rows\n",
      " county FY2023 Q1: 3233 rows\n",
      " county FY2023 Q2: 3233 rows\n",
      " county FY2023 Q3: 3232 rows\n",
      " county FY2023 Q4: 3233 rows\n",
      " county FY2024 Q2: 3234 rows\n",
      " county FY2024 Q1: 3234 rows\n",
      " county FY2024 Q3: 3234 rows\n",
      " county FY2024 Q4: 3234 rows\n",
      "Submitting 68 tasks for district (initial) with max_workers=4 \n",
      " district FY2008 Q1: 442 rows\n",
      " district FY2008 Q2: 442 rows\n",
      " district FY2008 Q3: 442 rows\n",
      " district FY2008 Q4: 442 rows\n",
      " district FY2009 Q1: 442 rows\n",
      " district FY2009 Q3: 442 rows\n",
      " district FY2009 Q2: 442 rows\n",
      " district FY2009 Q4: 442 rows\n",
      " district FY2010 Q1: 442 rows\n",
      " district FY2010 Q2: 442 rows\n",
      " district FY2010 Q3: 442 rows\n",
      " district FY2010 Q4: 442 rows\n",
      " district FY2011 Q1: 442 rows\n",
      " district FY2011 Q2: 442 rows\n",
      " district FY2011 Q3: 442 rows\n",
      " district FY2011 Q4: 442 rows\n",
      " district FY2012 Q3: 442 rows\n",
      " district FY2012 Q2: 442 rows\n",
      " district FY2012 Q1: 442 rows\n",
      " district FY2012 Q4: 442 rows\n",
      " district FY2013 Q1: 442 rows\n",
      " district FY2013 Q2: 442 rows\n",
      " district FY2013 Q3: 442 rows\n",
      " district FY2013 Q4: 442 rows\n",
      " district FY2014 Q1: 442 rows\n",
      " district FY2014 Q3: 442 rows\n",
      " district FY2014 Q2: 442 rows\n",
      " district FY2015 Q1: 442 rows\n",
      " district FY2014 Q4: 442 rows\n",
      " district FY2015 Q2: 442 rows\n",
      " district FY2015 Q3: 442 rows\n",
      " district FY2015 Q4: 442 rows\n",
      " district FY2016 Q1: 442 rows\n",
      " district FY2016 Q2: 442 rows\n",
      " district FY2016 Q3: 442 rows\n",
      " district FY2017 Q1: 442 rows\n",
      " district FY2016 Q4: 442 rows\n",
      " district FY2017 Q2: 442 rows\n",
      " district FY2017 Q4: 442 rows\n",
      " district FY2017 Q3: 442 rows\n",
      " district FY2018 Q1: 442 rows\n",
      " district FY2018 Q2: 442 rows\n",
      " district FY2018 Q3: 442 rows\n",
      " retry district FY2019 Q3 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2019 Q2 failed: HTTP 500 {'raw_text': '<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd\"><html><head><meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=0\"><style type=\"text/css\">       ht\n",
      " district FY2019 Q1: 442 rows\n",
      " retry district FY2019 Q4 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2020 Q1 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2018 Q4: 442 rows\n",
      " retry district FY2020 Q2 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2019 Q4 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2019 Q3: 442 rows\n",
      " retry district FY2020 Q3 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2019 Q4 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2020 Q4 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2020 Q2: 442 rows\n",
      " retry district FY2021 Q1 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2020 Q1: 442 rows\n",
      " retry district FY2020 Q3 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2021 Q2 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2020 Q4: 442 rows\n",
      " retry district FY2021 Q3 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2021 Q1: 442 rows\n",
      " district FY2020 Q3: 442 rows\n",
      " district FY2021 Q2: 442 rows\n",
      " retry district FY2021 Q4 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2022 Q1 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2022 Q2 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2021 Q3 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2021 Q4 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2022 Q2: 442 rows\n",
      " retry district FY2022 Q3 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2021 Q4 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2022 Q4 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2022 Q1: 442 rows\n",
      " district FY2021 Q3: 442 rows\n",
      " retry district FY2023 Q1 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2023 Q2 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2022 Q3 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2023 Q1 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2022 Q4: 442 rows\n",
      " retry district FY2023 Q3 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2023 Q2: 442 rows\n",
      " retry district FY2023 Q4 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2023 Q1 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2022 Q3: 442 rows\n",
      " retry district FY2024 Q1 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2024 Q2 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2023 Q3 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2024 Q1 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2024 Q2 (2/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2023 Q4: 442 rows\n",
      " retry district FY2024 Q3 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2024 Q1 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " retry district FY2024 Q4 (1/5) after ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      " district FY2024 Q2: 442 rows\n",
      " district FY2023 Q3: 442 rows\n",
      " district FY2024 Q3: 442 rows\n",
      " district FY2024 Q4: 442 rows\n"
     ]
    }
   ],
   "source": [
    "initial_run(START_FY, END_FY, LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Retry Run: Self-Healing Data Recovery\n",
    "\n",
    "**Intelligent failure recovery system** with idempotent retry operations:\n",
    "\n",
    "### Recovery Workflow\n",
    "1. **Failure Analysis**: Reads per-layer failure CSV files\n",
    "2. **Task Deduplication**: Creates unique set of (layer, FY, quarter) combinations\n",
    "3. **Selective Retry**: Only re-attempts previously failed tasks\n",
    "4. **Success Integration**: Merges recovered data into existing year files\n",
    "5. **Failure Update**: Maintains only currently failing tasks in CSV\n",
    "\n",
    "### Self-Healing Features\n",
    "- **Idempotent Operations**: Safe to run multiple times without data duplication\n",
    "- **Incremental Recovery**: Successful retries are immediately integrated\n",
    "- **Failure List Maintenance**: Automatically cleans up resolved failures\n",
    "- **Empty List Cleanup**: Deletes failure files when all tasks succeed\n",
    "\n",
    "### Retry Intelligence\n",
    "- **Fresh State**: Each retry gets clean session and connection pools\n",
    "- **Same Failover Logic**: Uses identical escalating failover as initial run\n",
    "- **Progress Visibility**: Shows retry-specific success/failure logging\n",
    "\n",
    "### Production Benefits\n",
    "- **Partial Collection Recovery**: Salvages successful data from interrupted runs\n",
    "- **Network Resilience**: Handles temporary API or network issues\n",
    "- **Operational Simplicity**: Single command recovers all pending failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1757444699546,
     "user": {
      "displayName": "Muniprathap Murari",
      "userId": "00205664222207817294"
     },
     "user_tz": 240
    },
    "id": "jGdm4TGKzXjp",
    "outputId": "1c05ee20-5cb0-4dc7-a3cd-07d3258cf9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No failures to retry for state.\n",
      " No failures to retry for country.\n",
      " No failures to retry for district.\n",
      " No failures to retry for county.\n"
     ]
    }
   ],
   "source": [
    "retry_run([\"state\", \"country\",\"district\",\"county\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Execution Examples & Usage Patterns\n",
    "\n",
    "**Practical examples** for running the geography data collection system:\n",
    "\n",
    "### Basic Execution\n",
    "```python\n",
    "# Full collection across all layers and fiscal years\n",
    "initial_run(START_FY, END_FY, LAYERS)\n",
    "```\n",
    "\n",
    "### Selective Layer Collection\n",
    "```python\n",
    "# Focus on specific geographic layers\n",
    "initial_run(2020, 2023, [\"state\", \"county\"])\n",
    "```\n",
    "\n",
    "### Recovery Operations\n",
    "```python\n",
    "# Retry only failed tasks from previous runs\n",
    "retry_run(LAYERS)\n",
    "```\n",
    "\n",
    "### Typical Workflow\n",
    "1. **Initial Collection**: Run `initial_run()` for comprehensive data gathering\n",
    "2. **Monitor Progress**: Watch console output for success/failure patterns\n",
    "3. **Recovery Phase**: Execute `retry_run()` to recover failed tasks\n",
    "4. **Validation**: Check output files and failure CSV status\n",
    "\n",
    "### Performance Tuning\n",
    "- **Layer Workers**: Adjust `LAYER_WORKERS` for optimal concurrency\n",
    "- **Timeout Settings**: Modify `TIMEOUT_S` based on API response patterns  \n",
    "- **Batch Throttling**: Add small delays between task submissions if hitting rate limits\n",
    "\n",
    "### Output Structure\n",
    "- **Per-Year Files**: `{layer}_FY{yyyy}.csv` for annual data\n",
    "- **All-Years Files**: `{layer}_ALL.csv` for complete datasets\n",
    "- **Failure Tracking**: `failures_{layer}.csv` for retry coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFNUUg5iz7nM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Production Deployment & Optimization\n",
    "\n",
    "**Enterprise deployment considerations** for large-scale geography data collection:\n",
    "\n",
    "### Performance Optimization\n",
    "- **Connection Tuning**: Adjust `pool_maxsize` based on concurrent requirements\n",
    "- **Worker Scaling**: Increase layer workers for higher throughput (monitor API rate limits)\n",
    "- **Timeout Configuration**: Use `timeout=(5, 60)` for fast connect, patient read\n",
    "- **Memory Management**: Monitor DataFrame accumulation in `by_layer_year`\n",
    "\n",
    "### Rate Limit Management\n",
    "```python\n",
    "# Add throttling for high-volume collection\n",
    "for task in tasks:\n",
    "    executor.submit(...)\n",
    "    time.sleep(0.02)  # 50 requests/second max\n",
    "```\n",
    "\n",
    "### Monitoring & Alerting\n",
    "- **Success Rate Tracking**: Monitor failure CSV file sizes\n",
    "- **Performance Metrics**: Log request latency and throughput\n",
    "- **Error Pattern Analysis**: Review common failure reasons\n",
    "- **Resource Utilization**: Monitor memory and connection pool usage\n",
    "\n",
    "### Scalability Considerations\n",
    "- **Horizontal Scaling**: Multiple processes for different fiscal year ranges\n",
    "- **Data Partitioning**: Separate collection by geographic region\n",
    "- **Storage Strategy**: Consider database storage for large datasets\n",
    "- **Network Resilience**: Deploy across multiple network zones\n",
    "\n",
    "### Troubleshooting Common Issues\n",
    "- **429 Rate Limits**: Reduce worker counts or add request throttling\n",
    "- **Memory Issues**: Implement streaming writes for large datasets\n",
    "- **Connection Exhaustion**: Increase pool sizes or reduce concurrency\n",
    "- **Timeout Errors**: Adjust timeout values for network conditions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNSQ+/pzFojKm9Pc3aCNPkP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}